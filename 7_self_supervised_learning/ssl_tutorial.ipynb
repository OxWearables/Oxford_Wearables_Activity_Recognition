{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEFORE WE START**: Make sure that any open notebooks that are no longer needed are shut down to free up compute resources (note that closing the tab is not enough!). This will reduce the risk of running into memory and/or performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Self-supervised learning tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook aims to provide you with a basic overview of self-supervised learning using accelerometers. There are three main components.\n",
    "\n",
    "1. Using the pre-trained model using self-supervision \n",
    "\n",
    "2. Surgical fine-tuning to enhance the downstream performance  \n",
    "\n",
    "3. Design novel self-supervised learning tasks for representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Fine tuning a pretrained model\n",
    "\n",
    "This notebook contains a minimal example of how to do fine-tuning on a pretrained PyTorch model. Fine-tuning means we take a pretrained model, and re-train it for a supervised learning task. The model pre-training in this example was done on 700,000 person-days of data on the UK Biobank. Details for the model development can be found in *[Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data](https://oxwearables.github.io/ssl-wearables/)*.\n",
    "\n",
    "The target downstream dataset is the Capture-24 Dataset with Walmsley labels ('light' 'moderate-vigorous' 'sedentary' 'sleep').\n",
    "\n",
    "Some helper functions and classes are loaded from `utils.py` and `data.py` in the utils folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Configure GPU\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from get_gpu import get_gpu\n",
    "device = get_gpu()\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from utils.data import NormalDataset, resize, get_inverse_class_weights\n",
    "from utils.utils import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load data\n",
    "\n",
    "Load the raw data in Numpy array format. You also need to change the paths inside this function to reflect your environment and dataset.\n",
    "\n",
    "## RAM issues\n",
    "Please make sure to close other running notebooks and restart the jupyter server before doing this practical. The full Capture-24 dataset has over 300,000 rows but we will only load a subset of 30,000 to avoid memory issues. You can try increasing this threshold (`max_size`), whilst monitoring RAM usage at the bottom of the screen. Your server is limited to 41GB RAM and if you go over it the kernel will crash (and restart). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_size = 30000\n",
    "\n",
    "def load_data():\n",
    "    root = '../processed_data'  # change this path if needed\n",
    "    X = np.load(os.path.join(root, 'X.npy'), mmap_mode='r')[ :max_size]  # accelerometer data\n",
    "    Y = np.load(os.path.join(root, 'Y.npy'))[ :max_size]  # true labels\n",
    "    pid = np.load(os.path.join(root, 'pid.npy'))[ :max_size]  # participant IDs\n",
    "    time = np.load(os.path.join(root, 'T.npy'))[:max_size]  # timestamps\n",
    "\n",
    "    anno_label_dict = pd.read_csv(\n",
    "        \"../capture24/annotation-label-dictionary.csv\",  # change this path if needed\n",
    "        index_col='annotation',\n",
    "        dtype='string'\n",
    "    )\n",
    "    Y = anno_label_dict.loc[Y, 'label:Willetts2018'].to_numpy()\n",
    "\n",
    "    print(f'X shape: {X.shape}')\n",
    "    print(f'Y shape: {Y.shape}')  # same shape as pid and time\n",
    "    print(f'Label distribution:\\n{pd.Series(Y).value_counts()}')\n",
    "\n",
    "    # The original labels in Y are in categorical format (e.g.: 'light', 'sleep', etc). PyTorch expects numerical labels (e.g.: 0, 1, etc).\n",
    "    # LabelEncoder transforms categorical labels -> numerical.\n",
    "    # After obtaining the test predictions, you can use le.inverse_transform(y) to go from numerical -> categorical (the fitted le object is returned at the end of this function)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(Y))\n",
    "\n",
    "    y = le.transform(Y)\n",
    "    print(f'Original labels: {le.classes_}')\n",
    "    print(f'Transformed labels: {le.transform(le.classes_)}')\n",
    "\n",
    "    # down sample if required.\n",
    "    # our pre-trained model expects windows of 30s at 30Hz = 900 samples\n",
    "    input_size = 900  # 30s at 30Hz\n",
    "\n",
    "    if X.shape[1] == input_size:\n",
    "        print(\"No need to downsample\")\n",
    "    else:\n",
    "        X = resize(X, input_size)\n",
    "\n",
    "    X = X.astype(\n",
    "        \"f4\"\n",
    "    )  # PyTorch defaults to float32\n",
    "\n",
    "    # generate train/test splits\n",
    "    folds = GroupShuffleSplit(\n",
    "        1, test_size=0.2, random_state=42\n",
    "    ).split(X, y, groups=pid)\n",
    "    train_idx, test_idx = next(folds)\n",
    "\n",
    "    x_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    group_test = pid[test_idx]\n",
    "\n",
    "    # further split train into train/val\n",
    "    X = X[train_idx]\n",
    "    y = y[train_idx]\n",
    "    pid = pid[train_idx]\n",
    "\n",
    "    folds = GroupShuffleSplit(\n",
    "        1, test_size=0.125, random_state=41\n",
    "    ).split(X, y, groups=pid)\n",
    "    train_idx, val_idx = next(folds)\n",
    "\n",
    "    x_train = X[train_idx]\n",
    "    x_val = X[val_idx]\n",
    "\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    group_train = pid[train_idx]\n",
    "    group_val = pid[val_idx]\n",
    "\n",
    "    return (\n",
    "        x_train, y_train, group_train,\n",
    "        x_val, y_val, group_val,\n",
    "        x_test, y_test, group_test,\n",
    "        le\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_val_test_data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now load the pre-trained self-supervised PyTorch model (a ResNet-18) from its GitHub repo (https://www.github.com/OxWearables/ssl-wearables).\n",
    "This repo exposes a Torch Hub API, and the model can be loaded using `torch.hub.load()`. Take note of the `pretrained=True` argument: this loads the pretrained weights into the model.\n",
    "\n",
    "Deep learning training loops benefit from batching, so called mini-batch training. It is faster than passing the whole dataset at once, and prevents getting stuck in local minima. The `NormalDataset` and `DataLoader` classes handle this process. `NormalDataset` implements as map-style dataset as described here https://pytorch.org/docs/stable/data.html. For the training dataset, we also enable augmentation by setting `transform=True`. Inspect the class to see how it works.\n",
    "\n",
    "The resulting `DataLoader` objects expose an iterable that will return a minibatch containing the accelerometer data, the ground truth label and the participant id. We later iterate over this dataloader during the training and testing loop using `enumerate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "repo = 'OxWearables/ssl-wearables'\n",
    "\n",
    "# load the pretrained model\n",
    "sslnet: nn.Module = torch.hub.load(repo, 'harnet30', trust_repo=True, class_num=6, pretrained=True, weights_only=False)\n",
    "sslnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "PyTorch models don't have `fit()` or `predict()` functions. We define the helper functions `train()`, `_validate_model()` and `predict()` ourselves. We also have a  `construct_dataloaders()` function to define the 3 DataLoaders for training, validation, and test sets. Inspect these to see what's going on.\n",
    "\n",
    "The model is then trained and tested. Training is done with an early-stopping mechanism. If the validation loss doesn't improve for 5 consecutive epochs, training is halted and the best weights prior to early-stopping are used. Inspect the `EarlyStopping` class in `utils.py` to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def construct_dataloaders(train_val_test_data):\n",
    "    # Unpack load_data return values. Discard last one if present (le; not needed in this method)\n",
    "    x_train, y_train, group_train, x_val, y_val, group_val, x_test, y_test, group_test, *_ = train_val_test_data\n",
    "    \n",
    "    train_dataset = NormalDataset(x_train, y_train, group_train, name=\"training\", transform=True)\n",
    "    val_dataset = NormalDataset(x_val, y_val, group_val, name=\"validation\")\n",
    "    test_dataset = NormalDataset(x_test, y_test, group_test, name=\"test\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, device, weights=None):\n",
    "    \"\"\"\n",
    "    Iterate over the training dataloader and train a pytorch model.\n",
    "    After each epoch, validate model and early stop when validation loss function bottoms out.\n",
    "\n",
    "    Trained model weights will be saved to disk (state_dict.pt).\n",
    "\n",
    "    :param nn.Module model: pytorch model\n",
    "    :param train_loader: training data loader\n",
    "    :param val_loader: validation data loader\n",
    "    :param str device: pytorch map device.\n",
    "    :param weights: training class weights (to enable weighted loss function)\n",
    "    \"\"\"\n",
    "\n",
    "    state_dict = 'state_dict.pt'\n",
    "\n",
    "    # REDUCE THIS IF YOU WANT TO SPEED UP THINGS, E.G. 2\n",
    "    num_epoch = 2\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.0001, amsgrad=True\n",
    "    )\n",
    "\n",
    "    if weights:\n",
    "        weights = torch.FloatTensor(weights).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "    else:\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=5, path=state_dict, verbose=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_acces = []\n",
    "        for i, (x, y, _) in enumerate(tqdm(train_loader)):\n",
    "            x.requires_grad_(True)\n",
    "            x = x.to(device, dtype=torch.float)\n",
    "            true_y = y.to(device, dtype=torch.long)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, true_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_y = torch.argmax(logits, dim=1)\n",
    "            train_acc = torch.sum(pred_y == true_y)\n",
    "            train_acc = train_acc / (pred_y.size()[0])\n",
    "\n",
    "            train_losses.append(loss.cpu().detach())\n",
    "            train_acces.append(train_acc.cpu().detach())\n",
    "\n",
    "        val_loss, val_acc = _validate_model(model, val_loader, device, loss_fn)\n",
    "\n",
    "        epoch_len = len(str(num_epoch))\n",
    "        print_msg = (\n",
    "            f\"[{epoch:>{epoch_len}}/{num_epoch:>{epoch_len}}] | \"\n",
    "            + f\"train_loss: {np.mean(train_losses):.3f} | \"\n",
    "            + f\"train_acc: {np.mean(train_acces):.3f} | \"\n",
    "            + f\"val_loss: {val_loss:.3f} | \"\n",
    "            + f\"val_acc: {val_acc:.2f}\"\n",
    "        )\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "        print(print_msg)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print('Early stopping')\n",
    "            print(f'SSLNet weights saved to {state_dict}')\n",
    "            break\n",
    "\n",
    "\n",
    "def _validate_model(model, val_loader, device, loss_fn):\n",
    "    \"\"\" Iterate over a validation data loader and return mean model loss and accuracy. \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    acces = []\n",
    "    for i, (x, y, _) in enumerate(val_loader):\n",
    "        with torch.inference_mode():\n",
    "            x = x.to(device, dtype=torch.float)\n",
    "            true_y = y.to(device, dtype=torch.long)\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, true_y)\n",
    "\n",
    "            pred_y = torch.argmax(logits, dim=1)\n",
    "\n",
    "            val_acc = torch.sum(pred_y == true_y)\n",
    "            val_acc = val_acc / (list(pred_y.size())[0])\n",
    "\n",
    "            losses.append(loss.cpu().detach())\n",
    "            acces.append(val_acc.cpu().detach())\n",
    "    losses = np.array(losses)\n",
    "    acces = np.array(acces)\n",
    "    return np.mean(losses), np.mean(acces)\n",
    "\n",
    "\n",
    "def predict(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Iterate over the dataloader and do inference with a pytorch model.\n",
    "\n",
    "    :param nn.Module model: pytorch Module\n",
    "    :param data_loader: pytorch dataloader\n",
    "    :param str device: pytorch map device\n",
    "    :return: true labels, model predictions, pids\n",
    "    :rtype: (np.ndarray, np.ndarray, np.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    predictions_list = []\n",
    "    true_list = []\n",
    "    pid_list = []\n",
    "    model.eval()\n",
    "\n",
    "    for i, (x, y, pid) in enumerate(tqdm(data_loader)):\n",
    "        with torch.inference_mode():\n",
    "            x = x.to(device, dtype=torch.float)\n",
    "            logits = model(x)\n",
    "            true_list.append(y)\n",
    "            pred_y = torch.argmax(logits, dim=1)\n",
    "            predictions_list.append(pred_y.cpu())\n",
    "            pid_list.extend(pid)\n",
    "    true_list = torch.cat(true_list)\n",
    "    predictions_list = torch.cat(predictions_list)\n",
    "\n",
    "    return (\n",
    "        torch.flatten(true_list).numpy(),\n",
    "        torch.flatten(predictions_list).numpy(),\n",
    "        np.array(pid_list),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model. The trained weights will be saved in the file 'state_dict.pt'\n",
    "train_loader, val_loader, test_loader = construct_dataloaders(train_val_test_data)\n",
    "# Need to access y_train to get class weights\n",
    "y_train = train_val_test_data[1] \n",
    "\n",
    "train(sslnet, train_loader, val_loader, device, get_inverse_class_weights(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# helper function to calculate classification performance scores: precision, recall, F1 and Kappa\n",
    "def classification_scores(y_test, y_test_pred):\n",
    "    import sklearn.metrics as metrics\n",
    "\n",
    "    cohen_kappa = metrics.cohen_kappa_score(y_test, y_test_pred)\n",
    "    precision = metrics.precision_score(\n",
    "        y_test, y_test_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    recall = metrics.recall_score(\n",
    "        y_test, y_test_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    f1 = metrics.f1_score(\n",
    "        y_test, y_test_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    data = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"kappa\": cohen_kappa,\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data, index=[0])  # use a dataframe because this prints nicely later\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load fine tuned weights (best weights prior to early-stopping) and do inference on the test set\n",
    "model_dict = torch.load('state_dict.pt', map_location=device)\n",
    "sslnet.load_state_dict(model_dict)\n",
    "\n",
    "y_test, y_test_pred, pid_test = predict(sslnet, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = classification_scores(y_test, y_test_pred)\n",
    "print(scores.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Surgical fine-tuning\n",
    "The fine-tuning in the previous part retrained all the layers in the model. Intuitively speaking, some information might be forgotten if we fine-tune on all the layers especially on small datasets.\n",
    "\n",
    "It is possible to freeze the weights of certain layers, and only fine-tune on selected layers. This is called surgical fine-tuning. Following the recent paper *[Surgical Fine-Tuning Improves Adaptation to Distribution Shifts](https://arxiv.org/abs/2210.11466)*, we would like to investigate the most optimal configuration.\n",
    "\n",
    "Below, we demonstrate how to access the list of model parameters and how to freeze certain weights during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "repo = 'OxWearables/ssl-wearables'\n",
    "\n",
    "# Load the model again. This resets the model with only the pretrained weights.\n",
    "sslnet: nn.Module = torch.hub.load(repo, 'harnet30', trust_repo=True, class_num=6, pretrained=True)\n",
    "sslnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To freeze network weights during training, you will need to know the name of the layers that you want to freeze, then set their `requires_grad` property to `False`. In this way, gradients will not be computed for those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for name, param in sslnet.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Freezing all the conv layers but the linear layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_bn_eval(m):\n",
    "    # keep the batch norm stats during forward pass\n",
    "    # see https://discuss.pytorch.org/t/how-to-freeze-bn-layers-while-training-the-rest-of-network-mean-and-var-wont-freeze/89736\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"BatchNorm1d\") != -1:\n",
    "        m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can explicitly verify parameter states like so:\n",
    "### This will ensure you know exactly which layers are frozen and which are trainable at any given time.\n",
    "\n",
    "for name, param in sslnet.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze all the \"feature_extractor\" (convolutional) layers\n",
    "for name, param in sslnet.named_parameters():\n",
    "    if \"feature_extractor\" in name:\n",
    "        param.requires_grad = False\n",
    "sslnet.apply(set_bn_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frozen_weights(sslnet):\n",
    "    count_frozen = 0\n",
    "    \n",
    "    for name, param in sslnet.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            count_frozen += 1\n",
    "            # Uncomment line below if you'd like to see which weights are frozen\n",
    "            #print(name, param.requires_grad)\n",
    "            \n",
    "    return count_frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Weights being frozen: %d\" % count_frozen_weights(sslnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Freezing the second residual block in the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's reset the model to its default state: set all weights to have param.requires_grad == True\n",
    "\n",
    "def reset_state(sslnet):\n",
    "    # Reset requires_grad for all parameters in the model\n",
    "    for name, param in sslnet.named_parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    print(\"All parameters have been reset to requires_grad == True\")\n",
    "\n",
    "    return sslnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sslnet = reset_state(sslnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's freeze the specified layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for name, param in sslnet.named_parameters():\n",
    "    if \"feature_extractor.layer1.2\" in name:  # '2' corresponds to the second ResBlock\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Weights being frozen: %d\" % count_frozen_weights(sslnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 2-1. Does the performance change if you only fine-tune the first layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 2-2. Does the performance change if you only fine-tune the middle layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 2-3 [challenge]. What fine-tuning configurations might yield the best performance?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Design novel self-supervised learning tasks for representation learning\n",
    "\n",
    "Self-supervised learning means we train a model with self-derived labels. This is typically done on datasets that lack ground truth labels. We may not know the true ground truth label of a piece of data, but we can try to derive a label ourselves. For accelerometer signals, we can transform the signal in a certain way and then use the type of transformation as the label (for example: 'rotation'). The model is then trained to predict the transformation label. This may seem useless, and by itself it is, but when followed up with a supervised learning task (the fine-tuning) it can improve performance.\n",
    "\n",
    "Once you know what your self-supervised learning task is, the implementation is usually easy because all you have to do is to change the label vector as x stays the same. The training pipeline also shouldn't change much. That's how many people differentiate between unsupervised learning and self-supervised learning when training using labelled data. **If your training pipeline stays the same, then your technique is *self-supervised*. If your pipeline changes, then your technique is *unsupervised*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reverse(input_x, label):\n",
    "    # label = 0: no reversal\n",
    "    # label = 1: reversal \n",
    "    if label == 0:\n",
    "        return input_x\n",
    "    else:\n",
    "        return np.flip(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reverse_probability = 0.5\n",
    "x_train = train_val_test_data[0]\n",
    "new_X = []\n",
    "new_Y = []\n",
    "for i in range(len(x_train)):\n",
    "    current_x = x_train[i]\n",
    "    if np.random.rand() > reverse_probability:\n",
    "        current_y = 1\n",
    "    else:\n",
    "        current_y = 0\n",
    "    new_x = reverse(current_x, current_y)\n",
    "    new_X.append(new_x)\n",
    "    \n",
    "    new_Y.append(current_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_X = np.array(new_X)\n",
    "new_Y = np.array(new_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 3-1: Can you pretrain the SLL model yourself using the Capture-24 data using the reverse task above and see if it helps with the downstream task of activity recognition with the original Capture-24 classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 3-2: Can you implement any other self-supervised tasks that you think might help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1224fc7bbcf00895b4029033e99b6a45061f4970375018967a2862735a536e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
