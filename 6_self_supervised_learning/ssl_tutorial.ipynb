{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Self-supervised learning tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook aims to provide you with a basic overview of self-supervised learning using accelerometers. There are three main components.\n",
    "\n",
    "1. Using the pre-trained model using self-supervision \n",
    "\n",
    "2. Surgical fine-tuning to enhance the downstream performance  \n",
    "\n",
    "3. Design novel self-supervised learning tasks for representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Fine tuning a pretrained model\n",
    "\n",
    "This notebook contains a minimal example of how to do fine-tuning on a pretrained PyTorch model. Fine-tuning means we take a pretrained model, and re-train it for a supervised learning task. The model pre-training in this example was done on 700,000 person-days of data on the UK Biobank. Details for the model development can be found in *[Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data](https://oxwearables.github.io/ssl-wearables/)*.\n",
    "\n",
    "The target downstream dataset is the Capture-24 Dataset with Walmsley labels ('light' 'moderate-vigorous' 'sedentary' 'sleep'). Generate a `Y.npy` yourself with the Walmsley labels using the earlier notebooks.\n",
    "\n",
    "Some helper functions and classes are loaded from `utils.py` and `data.py` in the utils folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from utils.data import NormalDataset, resize, get_inverse_class_weights\n",
    "from utils.utils import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the raw data in Numpy array format. You also need to change the paths inside this function to reflect your environment and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    root = '/Users/hangy/Desktop/processed_data'\n",
    "\n",
    "    X = np.load(os.path.join(root, 'X.npy'), mmap_mode='r')  # accelerometer data\n",
    "    Y = np.load(os.path.join(root, 'Y.npy'))  # true labels\n",
    "    pid = np.load(os.path.join(root, 'pid.npy'))  # participant IDs\n",
    "    time = np.load(os.path.join(root, 'T.npy'))  # timestamps\n",
    "\n",
    "    print(f'X shape: {X.shape}')\n",
    "    print(f'Y shape: {Y.shape}')  # same shape as pid and time\n",
    "    print(f'Label distribution:\\n{pd.Series(Y).value_counts()}')\n",
    "\n",
    "    # The original labels in Y are in categorical format (e.g.: 'light', 'sleep', etc). PyTorch expects numerical labels (e.g.: 0, 1, etc).\n",
    "    # LabelEncoder transforms categorical labels -> numerical.\n",
    "    # After obtaining the test predictions, you can use le.inverse_transform(y) to go from numerical -> categorical (the fitted le object is returned at the end of this function)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(Y))\n",
    "\n",
    "    y = le.transform(Y)\n",
    "    print(f'Original labels: {le.classes_}')\n",
    "    print(f'Transformed labels: {le.transform(le.classes_)}')\n",
    "\n",
    "    # down sample if required.\n",
    "    # our pre-trained model expects windows of 30s at 30Hz = 900 samples\n",
    "    input_size = 900  # 10s at 30Hz\n",
    "\n",
    "    if X.shape[1] == input_size:\n",
    "        print(\"No need to downsample\")\n",
    "    else:\n",
    "        X = resize(X, input_size)\n",
    "\n",
    "    X = X.astype(\n",
    "        \"f4\"\n",
    "    )  # PyTorch defaults to float32\n",
    "\n",
    "    # generate train/test splits\n",
    "    folds = GroupShuffleSplit(\n",
    "        1, test_size=0.2, random_state=42\n",
    "    ).split(X, y, groups=pid)\n",
    "    train_idx, test_idx = next(folds)\n",
    "\n",
    "    x_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    time_test = time[test_idx]\n",
    "    group_test = pid[test_idx]\n",
    "\n",
    "    # further split train into train/val\n",
    "    X = X[train_idx]\n",
    "    y = y[train_idx]\n",
    "    pid = pid[train_idx]\n",
    "    time = time[train_idx]\n",
    "\n",
    "    folds = GroupShuffleSplit(\n",
    "        1, test_size=0.125, random_state=41\n",
    "    ).split(X, y, groups=pid)\n",
    "    train_idx, val_idx = next(folds)\n",
    "\n",
    "    x_train = X[train_idx]\n",
    "    x_val = X[val_idx]\n",
    "\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    time_train = time[train_idx]\n",
    "    time_val = time[val_idx]\n",
    "\n",
    "    group_train = pid[train_idx]\n",
    "    group_val = pid[val_idx]\n",
    "\n",
    "    return (\n",
    "        x_train, y_train, group_train, time_train,\n",
    "        x_val, y_val, group_val, time_val,\n",
    "        x_test, y_test, group_test, time_test,\n",
    "        le\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (294659, 900, 3)\n",
      "Y shape: (294659,)\n",
      "Label distribution:\n",
      "sedentary            116544\n",
      "sleep                113355\n",
      "light                 50822\n",
      "moderate-vigorous     13938\n",
      "dtype: int64\n",
      "Original labels: ['light' 'moderate-vigorous' 'sedentary' 'sleep']\n",
      "Transformed labels: [0 1 2 3]\n",
      "No need to downsample\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    x_train, y_train, group_train, time_train,\n",
    "    x_val, y_val, group_val, time_val,\n",
    "    x_test, y_test, group_test, time_test,\n",
    "    le\n",
    ") = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now load the pre-trained self-supervised PyTorch model (a ResNet-18) from its GitHub repo (https://www.github.com/OxWearables/ssl-wearables).\n",
    "This repo exposes a Torch Hub API, and the model can be loaded using `torch.hub.load()`. Take note of the `pretrained=True` argument: this loads the pretrained weights into the model.\n",
    "\n",
    "Deep learning training loops benefit from batching, so called mini-batch training. It is faster than passing the whole dataset at once, and prevents getting stuck in local minima. The `NormalDataset` and `DataLoader` classes handle this process. `NormalDataset` implements as map-style dataset as described here https://pytorch.org/docs/stable/data.html. For the training dataset, we also enable augmentation by setting `transform=True`. Inspect the class to see how it works.\n",
    "\n",
    "The resulting `DataLoader` objects expose an iterable that will return a minibatch containing the accelerometer data, the ground truth label and the participant id. We later iterate over this dataloader during the training and testing loop using `enumerate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/azw524/.cache/torch/hub/OxWearables_ssl-wearables_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 Weights loaded\n",
      "training set sample count : 206503\n",
      "validation set sample count : 28664\n",
      "test set sample count : 59492\n"
     ]
    }
   ],
   "source": [
    "repo = 'OxWearables/ssl-wearables'\n",
    "my_device = 'cuda:0'\n",
    "\n",
    "# load the pretrained model\n",
    "sslnet: nn.Module = torch.hub.load(repo, 'harnet30', trust_repo=True, class_num=4, pretrained=True)\n",
    "sslnet.to(my_device)\n",
    "\n",
    "# construct dataloaders\n",
    "train_dataset = NormalDataset(x_train, y_train, group_train, name=\"training\", transform=True)\n",
    "val_dataset = NormalDataset(x_val, y_val, group_val, name=\"validation\")\n",
    "test_dataset = NormalDataset(x_test, y_test, group_test, name=\"test\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2000,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2000,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2000,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "PyTorch models don't have `fit()` or `predict()` functions. We define the helper functions `train()`, `_validate_model()` and `predict()` ourselves. Inspect these to see what's going on.\n",
    "\n",
    "The model is then trained and tested. Training is done with an early-stopping mechanism. If the validation loss doesn't improve for 5 consecutive epochs, training is halted and the best weights prior to early-stopping are used. Inspect the `EarlyStopping` class in `utils.py` to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, my_device, weights=None):\n",
    "    \"\"\"\n",
    "    Iterate over the training dataloader and train a pytorch model.\n",
    "    After each epoch, validate model and early stop when validation loss function bottoms out.\n",
    "\n",
    "    Trained model weights will be saved to disk (state_dict.pt).\n",
    "\n",
    "    :param nn.Module model: pytorch model\n",
    "    :param train_loader: training data loader\n",
    "    :param val_loader: validation data loader\n",
    "    :param str my_device: pytorch map device.\n",
    "    :param weights: training class weights (to enable weighted loss function)\n",
    "    \"\"\"\n",
    "\n",
    "    state_dict = 'state_dict.pt'\n",
    "    num_epoch = 100\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.0001, amsgrad=True\n",
    "    )\n",
    "\n",
    "    if weights:\n",
    "        weights = torch.FloatTensor(weights).to(my_device)\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "    else:\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=5, path=state_dict, verbose=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_acces = []\n",
    "        for i, (x, y, _) in enumerate(tqdm(train_loader)):\n",
    "            x.requires_grad_(True)\n",
    "            x = x.to(my_device, dtype=torch.float)\n",
    "            true_y = y.to(my_device, dtype=torch.long)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, true_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_y = torch.argmax(logits, dim=1)\n",
    "            train_acc = torch.sum(pred_y == true_y)\n",
    "            train_acc = train_acc / (pred_y.size()[0])\n",
    "\n",
    "            train_losses.append(loss.cpu().detach())\n",
    "            train_acces.append(train_acc.cpu().detach())\n",
    "\n",
    "        val_loss, val_acc = _validate_model(model, val_loader, my_device, loss_fn)\n",
    "\n",
    "        epoch_len = len(str(num_epoch))\n",
    "        print_msg = (\n",
    "            f\"[{epoch:>{epoch_len}}/{num_epoch:>{epoch_len}}] | \"\n",
    "            + f\"train_loss: {np.mean(train_losses):.3f} | \"\n",
    "            + f\"train_acc: {np.mean(train_acces):.3f} | \"\n",
    "            + f\"val_loss: {val_loss:.3f} | \"\n",
    "            + f\"val_acc: {val_acc:.2f}\"\n",
    "        )\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "        print(print_msg)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print('Early stopping')\n",
    "            print(f'SSLNet weights saved to {state_dict}')\n",
    "            break\n",
    "\n",
    "\n",
    "def _validate_model(model, val_loader, my_device, loss_fn):\n",
    "    \"\"\" Iterate over a validation data loader and return mean model loss and accuracy. \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    acces = []\n",
    "    for i, (x, y, _) in enumerate(val_loader):\n",
    "        with torch.inference_mode():\n",
    "            x = x.to(my_device, dtype=torch.float)\n",
    "            true_y = y.to(my_device, dtype=torch.long)\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, true_y)\n",
    "\n",
    "            pred_y = torch.argmax(logits, dim=1)\n",
    "\n",
    "            val_acc = torch.sum(pred_y == true_y)\n",
    "            val_acc = val_acc / (list(pred_y.size())[0])\n",
    "\n",
    "            losses.append(loss.cpu().detach())\n",
    "            acces.append(val_acc.cpu().detach())\n",
    "    losses = np.array(losses)\n",
    "    acces = np.array(acces)\n",
    "    return np.mean(losses), np.mean(acces)\n",
    "\n",
    "\n",
    "def predict(model, data_loader, my_device):\n",
    "    \"\"\"\n",
    "    Iterate over the dataloader and do inference with a pytorch model.\n",
    "\n",
    "    :param nn.Module model: pytorch Module\n",
    "    :param data_loader: pytorch dataloader\n",
    "    :param str my_device: pytorch map device\n",
    "    :return: true labels, model predictions, pids\n",
    "    :rtype: (np.ndarray, np.ndarray, np.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    predictions_list = []\n",
    "    true_list = []\n",
    "    pid_list = []\n",
    "    model.eval()\n",
    "\n",
    "    for i, (x, y, pid) in enumerate(tqdm(data_loader)):\n",
    "        with torch.inference_mode():\n",
    "            x = x.to(my_device, dtype=torch.float)\n",
    "            logits = model(x)\n",
    "            true_list.append(y)\n",
    "            pred_y = torch.argmax(logits, dim=1)\n",
    "            predictions_list.append(pred_y.cpu())\n",
    "            pid_list.extend(pid)\n",
    "    true_list = torch.cat(true_list)\n",
    "    predictions_list = torch.cat(predictions_list)\n",
    "\n",
    "    return (\n",
    "        torch.flatten(true_list).numpy(),\n",
    "        torch.flatten(predictions_list).numpy(),\n",
    "        np.array(pid_list),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model. The trained weights will be saved in the file 'state_dict.pt'\n",
    "train(sslnet, train_loader, val_loader, my_device, get_inverse_class_weights(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# helper function to calculate classification performance scores: precision, recall, F1 and Kappa\n",
    "def classification_scores(y_test, y_test_pred):\n",
    "    import sklearn.metrics as metrics\n",
    "\n",
    "    cohen_kappa = metrics.cohen_kappa_score(y_test, y_test_pred)\n",
    "    precision = metrics.precision_score(\n",
    "        y_test, y_test_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    recall = metrics.recall_score(\n",
    "        y_test, y_test_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    f1 = metrics.f1_score(\n",
    "        y_test, y_test_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    data = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"kappa\": cohen_kappa,\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data, index=[0])  # use a dataframe because this prints nicely later\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# load fine tuned weights (best weights prior to early-stopping) and do inference on the test set\n",
    "model_dict = torch.load('state_dict.pt', map_location=my_device)\n",
    "sslnet.load_state_dict(model_dict)\n",
    "\n",
    "y_test, y_test_pred, pid_test = predict(sslnet, test_loader, my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   precision  recall     f1  kappa\n",
      "0      0.823   0.836  0.829  0.805\n"
     ]
    }
   ],
   "source": [
    "scores = classification_scores(y_test, y_test_pred)\n",
    "print(scores.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Surgical fine-tuning\n",
    "The fine-tuning in the previous part retrained all the layers in the model. Intuitively speaking, some information might be forgotten if we fine-tune on all the layers especially on small datasets.\n",
    "\n",
    "It is possible to freeze the weights of certain layers, and only fine-tune on selected layers. This is called surgical fine-tuning. Following the recent paper *[Surgical Fine-Tuning Improves Adaptation to Distribution Shifts](https://arxiv.org/abs/2210.11466)*, we would like to investigate the most optimal configuration.\n",
    "\n",
    "Below, we demonstrate how to access the list of model parameters and how to freeze certain weights during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "repo = 'OxWearables/ssl-wearables'\n",
    "my_device = 'cuda:0'\n",
    "\n",
    "# Load the model again. This resets the model with only the pretrained weights.\n",
    "sslnet: nn.Module = torch.hub.load(repo, 'harnet30', trust_repo=True, class_num=4, pretrained=True)\n",
    "sslnet.to(my_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To freeze network weights during training, you will need to know the name of the layers that you want to freeze, then set their `requires_grad` property to `False`. In this way, gradients will not be computed for those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_extractor.layer1.0.weight\n",
      "feature_extractor.layer1.1.bn1.weight\n",
      "feature_extractor.layer1.1.bn1.bias\n",
      "feature_extractor.layer1.1.bn2.weight\n",
      "feature_extractor.layer1.1.bn2.bias\n",
      "feature_extractor.layer1.1.conv1.weight\n",
      "feature_extractor.layer1.1.conv2.weight\n",
      "feature_extractor.layer1.2.bn1.weight\n",
      "feature_extractor.layer1.2.bn1.bias\n",
      "feature_extractor.layer1.2.bn2.weight\n",
      "feature_extractor.layer1.2.bn2.bias\n",
      "feature_extractor.layer1.2.conv1.weight\n",
      "feature_extractor.layer1.2.conv2.weight\n",
      "feature_extractor.layer1.3.weight\n",
      "feature_extractor.layer1.3.bias\n",
      "feature_extractor.layer2.0.weight\n",
      "feature_extractor.layer2.1.bn1.weight\n",
      "feature_extractor.layer2.1.bn1.bias\n",
      "feature_extractor.layer2.1.bn2.weight\n",
      "feature_extractor.layer2.1.bn2.bias\n",
      "feature_extractor.layer2.1.conv1.weight\n",
      "feature_extractor.layer2.1.conv2.weight\n",
      "feature_extractor.layer2.2.bn1.weight\n",
      "feature_extractor.layer2.2.bn1.bias\n",
      "feature_extractor.layer2.2.bn2.weight\n",
      "feature_extractor.layer2.2.bn2.bias\n",
      "feature_extractor.layer2.2.conv1.weight\n",
      "feature_extractor.layer2.2.conv2.weight\n",
      "feature_extractor.layer2.3.weight\n",
      "feature_extractor.layer2.3.bias\n",
      "feature_extractor.layer3.0.weight\n",
      "feature_extractor.layer3.1.bn1.weight\n",
      "feature_extractor.layer3.1.bn1.bias\n",
      "feature_extractor.layer3.1.bn2.weight\n",
      "feature_extractor.layer3.1.bn2.bias\n",
      "feature_extractor.layer3.1.conv1.weight\n",
      "feature_extractor.layer3.1.conv2.weight\n",
      "feature_extractor.layer3.2.bn1.weight\n",
      "feature_extractor.layer3.2.bn1.bias\n",
      "feature_extractor.layer3.2.bn2.weight\n",
      "feature_extractor.layer3.2.bn2.bias\n",
      "feature_extractor.layer3.2.conv1.weight\n",
      "feature_extractor.layer3.2.conv2.weight\n",
      "feature_extractor.layer3.3.weight\n",
      "feature_extractor.layer3.3.bias\n",
      "feature_extractor.layer4.0.weight\n",
      "feature_extractor.layer4.1.bn1.weight\n",
      "feature_extractor.layer4.1.bn1.bias\n",
      "feature_extractor.layer4.1.bn2.weight\n",
      "feature_extractor.layer4.1.bn2.bias\n",
      "feature_extractor.layer4.1.conv1.weight\n",
      "feature_extractor.layer4.1.conv2.weight\n",
      "feature_extractor.layer4.2.bn1.weight\n",
      "feature_extractor.layer4.2.bn1.bias\n",
      "feature_extractor.layer4.2.bn2.weight\n",
      "feature_extractor.layer4.2.bn2.bias\n",
      "feature_extractor.layer4.2.conv1.weight\n",
      "feature_extractor.layer4.2.conv2.weight\n",
      "feature_extractor.layer4.3.weight\n",
      "feature_extractor.layer4.3.bias\n",
      "feature_extractor.layer5.0.weight\n",
      "feature_extractor.layer5.1.weight\n",
      "feature_extractor.layer5.1.bias\n",
      "classifier.linear1.weight\n",
      "classifier.linear1.bias\n",
      "classifier.linear2.weight\n",
      "classifier.linear2.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in sslnet.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Freezing all the conv layers but the linear layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_bn_eval(m):\n",
    "    # keep the batch norm stats during forward pass\n",
    "    # see https://discuss.pytorch.org/t/how-to-freeze-bn-layers-while-training-the-rest-of-network-mean-and-var-wont-freeze/89736\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"BatchNorm1d\") != -1:\n",
    "        m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "name_idx = 0\n",
    "for name, param in sslnet.named_parameters():\n",
    "    if name.split(\".\")[name_idx] == \"feature_extractor\":\n",
    "        param.requires_grad = False\n",
    "        i += 1\n",
    "sslnet.apply(set_bn_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights being frozen: 63\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights being frozen: %d\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Freezing all the weights layers in the first residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "name_idx = 1\n",
    "for name, param in sslnet.named_parameters():\n",
    "    if name.split(\".\")[name_idx] == \"layer1\":\n",
    "        param.requires_grad = False\n",
    "        i += 1\n",
    "sslnet.apply(set_bn_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights being frozen: 15\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights being frozen: %d\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Complete the following exercises by freezing parts of the model and fine-tuning the model on Capture24."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 2-1. Does the performance change if you only fine-tune the first layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 2-2. Does the performance change if you only fine-tune the middle layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 2-3 [challenge]. What fine-tuning configurations might yield the best performance?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Design novel self-supervised learning tasks for representation learning\n",
    "\n",
    "Self-supervised learning means we train a model with self-derived labels. This is typically done on datasets that lack ground truth labels. We may not know the true ground truth label of a piece of data, but we can try to derive a label ourselves. For accelerometer signals, we can transform the signal in a certain way and then use the type of transformation as the label (for example: 'rotation'). The model is then trained to predict the transformation label. This may seem useless, and by itself it is, but when followed up with a supervised learning task (the fine-tuning) it can improve performance.\n",
    "\n",
    "Once you know what your self-supervised learning task is, the implementation is usually easy because all you have to do is to change the label vector as x stays the same. The training pipeline also shouldn't change much. That's how many people differentiate between unsupervised learning and self-supervised learning when training using labelled data. **If your training pipeline stays the same, then your technique is *self-supervised*. If your pipeline changes, then your technique is *unsupervised*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reverse(input_x, label):\n",
    "    # label = 0: no reversal\n",
    "    # label = 1: reversal \n",
    "    if label == 0:\n",
    "        return input_x\n",
    "    else:\n",
    "        return np.flip(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reverse_probability = 0.5\n",
    "new_X = []\n",
    "new_Y = []\n",
    "for i in range(len(x_train)):\n",
    "    current_x = x_train[i]\n",
    "    if np.random.rand() > reverse_probability:\n",
    "        current_y = 1\n",
    "    else:\n",
    "        current_y = 0\n",
    "    new_x = reverse(current_x, current_y)\n",
    "    new_X.append(new_x)\n",
    "    \n",
    "    new_Y.append(current_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_X = np.array(new_X)\n",
    "new_Y = np.array(new_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 3-1: Can you pretrain the capture-24 using the reverse task above and see if it helps with the activity recognition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 3-2: Can you implement any other self-supervised tasks that you think might help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1224fc7bbcf00895b4029033e99b6a45061f4970375018967a2862735a536e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}