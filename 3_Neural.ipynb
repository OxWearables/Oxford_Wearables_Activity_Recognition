{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks in activity recognition\n",
    "\n",
    "Engineering effective features is one of the most time-consuming parts of\n",
    "machine learning.\n",
    "The appeal of neural networks is that feature enginnering is integrated into the\n",
    "training process &mdash; they automatically engineer features that are relevant\n",
    "for the learning task directly from the raw representation of the data\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Grab a GPU if there is one\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using {} device: {}\".format(device, torch.cuda.current_device()))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of processed_data/\n",
      "['T.npy', 'pid.npy', 'X.npy', 'Y.npy']\n"
     ]
    }
   ],
   "source": [
    "# Path to your extracted windows\n",
    "DATASET_PATH = 'processed_data/'\n",
    "print(f'Content of {DATASET_PATH}')\n",
    "print(os.listdir(DATASET_PATH))\n",
    "\n",
    "X = np.load(DATASET_PATH+'X.npy', mmap_mode='r')\n",
    "Y = np.load(DATASET_PATH+'Y.npy')\n",
    "T = np.load(DATASET_PATH+'T.npy')\n",
    "pid = np.load(DATASET_PATH+'pid.npy')\n",
    "\n",
    "# As before, let's map the text annotations to simplified labels\n",
    "ANNO_LABEL_DICT_PATH = 'capture24/annotation-label-dictionary.csv'\n",
    "anno_label_dict = pd.read_csv(ANNO_LABEL_DICT_PATH, index_col='annotation', dtype='string')\n",
    "Y = anno_label_dict.loc[Y, 'label:Willetts2018'].to_numpy()\n",
    "\n",
    "# Transform to numeric\n",
    "le = LabelEncoder().fit(Y)\n",
    "Y = le.transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (205969, 3000, 3)\n",
      "Shape of X_test: (101276, 3000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Hold out participants P101-P151 for testing (51 participants)\n",
    "test_ids = [f'P{i}' for i in range(101,152)]\n",
    "mask_test = np.isin(pid, test_ids)\n",
    "mask_train = ~mask_test\n",
    "X_train, Y_train, T_train, pid_train = \\\n",
    "    X[mask_train], Y[mask_train], T[mask_train], pid[mask_train]\n",
    "X_test, Y_test, T_test, pid_test = \\\n",
    "    X[mask_test], Y[mask_test], T[mask_test], pid[mask_test]\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design\n",
    "\n",
    "As a baseline, let's use a convolutional neural network (CNN) with a\n",
    "typical pyramid-like structure. The input to the network is a `(N,3,3000)`\n",
    "array, corresponding to `N` windows of raw tri-axial accelerometer measures.\n",
    "Note the transposed format `(3,3000)` instead of `(3000,3)`; this *channels\n",
    "first* format is the default in PyTorch.\n",
    "\n",
    "The output of the CNN is a `(N,num_labels)` array where each row contains\n",
    "predicted unnormalized class scores or *logits*; pass each row to a softmax\n",
    "if you want to convert it to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Module):\n",
    "    ''' Convolution + batch normalization + ReLU is a common trio '''\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels,\n",
    "        kernel_size=3, stride=1, padding=1, bias=True\n",
    "    ):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels,\n",
    "                kernel_size, stride, padding, bias=bias),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    ''' Typical CNN design with pyramid-like structure '''\n",
    "    def __init__(self, output_size=5, in_channels=3, num_filters_init=8):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            ConvBNReLU(in_channels, num_filters_init,\n",
    "            8, 4, 2, bias=False),  # 1500 -> 750\n",
    "            ConvBNReLU(num_filters_init, num_filters_init*2,\n",
    "            6, 4, 2, bias=False),  # 750 -> 188\n",
    "            ConvBNReLU(num_filters_init*2, num_filters_init*4,\n",
    "            8, 4, 2, bias=False),  # 188 -> 47\n",
    "            ConvBNReLU(num_filters_init*4, num_filters_init*8,\n",
    "            3, 2, 1, bias=False),  # 47 -> 24\n",
    "            ConvBNReLU(num_filters_init*8, num_filters_init*16,\n",
    "            4, 2, 1, bias=False),  # 24 -> 12\n",
    "            ConvBNReLU(num_filters_init*16, num_filters_init*32,\n",
    "            4, 2, 1, bias=False),  # 12 -> 6\n",
    "            ConvBNReLU(num_filters_init*32, num_filters_init*64,\n",
    "            6, 1, 0, bias=False),  # 6 -> 1\n",
    "            nn.Conv1d(num_filters_init*64, output_size,\n",
    "            1, 1, 0, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x).view(x.shape[0],-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(X, y=None, batch_size=1, shuffle=False):\n",
    "    ''' Create a (batch) iterator over the dataset. Alternatively, you can use\n",
    "    PyTorch's Dataset and DataLoader classes -- See\n",
    "    https://pytorch.org/tutorials/beginner/data_loading_tutorial.html '''\n",
    "    if shuffle:\n",
    "        idxs = np.random.permutation(np.arange(len(X)))\n",
    "    else:\n",
    "        idxs = np.arange(len(X))\n",
    "    for i in range(0, len(idxs), batch_size):\n",
    "        idxs_batch = idxs[i:i+batch_size]\n",
    "        X_batch = X[idxs_batch].astype('f4')  # PyTorch defaults to float32\n",
    "        X_batch = np.transpose(X_batch, (0,2,1))  # channels first: (N,M,3) -> (N,3,M). PyTorch uses channel first format\n",
    "        X_batch = torch.from_numpy(X_batch)\n",
    "        if y is None:\n",
    "            yield X_batch\n",
    "        else:\n",
    "            y_batch = y[idxs_batch]\n",
    "            y_batch = torch.from_numpy(y_batch)\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "\n",
    "def forward_by_batches(cnn, X):\n",
    "    ''' Forward pass model on a dataset.\n",
    "    Do this by batches so that we don't blow up the memory. '''\n",
    "    Y = []\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        for x in create_dataloader(X, batch_size=1024, shuffle=False):  # do not shuffle here!\n",
    "            x = x.to(device)\n",
    "            Y.append(cnn(x))\n",
    "    cnn.train()\n",
    "    Y = torch.cat(Y)\n",
    "    return Y\n",
    "\n",
    "\n",
    "def evaluate_model(cnn, X, Y):\n",
    "    Y_pred = forward_by_batches(cnn, X)  # scores\n",
    "    loss = F.cross_entropy(Y_pred, torch.from_numpy(Y).type(torch.int64).to(device)).item()\n",
    "\n",
    "    Y_pred = F.softmax(Y_pred, dim=1)  # convert to probabilities\n",
    "    Y_pred = torch.argmax(Y_pred, dim=1)  # convert to classes\n",
    "    Y_pred = Y_pred.cpu().numpy()  # cast to numpy array\n",
    "    kappa = metrics.cohen_kappa_score(Y, Y_pred)\n",
    "\n",
    "    return {'loss':loss, 'kappa':kappa, 'Y_pred':Y_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Hyperparameters, model instantiation, loss function and optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (cnn): Sequential(\n",
      "    (0): ConvBNReLU(\n",
      "      (main): Sequential(\n",
      "        (0): Conv1d(3, 32, kernel_size=(8,), stride=(4,), padding=(2,), bias=False)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvBNReLU(\n",
      "      (main): Sequential(\n",
      "        (0): Conv1d(32, 64, kernel_size=(6,), stride=(4,), padding=(2,), bias=False)\n",
      "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ConvBNReLU(\n",
      "      (main): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(8,), stride=(4,), padding=(2,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): ConvBNReLU(\n",
      "      (main): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (4): ConvBNReLU(\n",
      "      (main): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): ConvBNReLU(\n",
      "      (main): Sequential(\n",
      "        (0): Conv1d(512, 1024, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "        (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): ConvBNReLU(\n",
      "      (main): Sequential(\n",
      "        (0): Conv1d(1024, 2048, kernel_size=(6,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Conv1d(2048, 6, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_filters_init = 32  # initial num of filters -- see class definition\n",
    "in_channels = 3  # num of channels of the signal -- equal to 3 for our raw triaxial timeseries\n",
    "output_size = len(np.unique(Y))  # num of classes (sleep, sedentary, etc...)\n",
    "num_epoch = 1  # num of epochs (full loops though the training set)\n",
    "lr = 3e-4  # learning rate\n",
    "batch_size = 32  # size of the mini-batch\n",
    "\n",
    "cnn = CNN(\n",
    "    output_size=output_size,\n",
    "    in_channels=in_channels,\n",
    "    num_filters_init=num_filters_init\n",
    ").to(device)\n",
    "print(cnn)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15544083071f40d4bdb28f735be0f214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m loss_history_train\u001b[38;5;241m.\u001b[39mappend(utils\u001b[38;5;241m.\u001b[39mewm(losses))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Logging -- evalutate performance on test set\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m loss_history_test\u001b[38;5;241m.\u001b[39mappend(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     28\u001b[0m kappa_history_test\u001b[38;5;241m.\u001b[39mappend(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkappa\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(cnn, X, Y)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(cnn, X, Y):\n\u001b[0;32m---> 37\u001b[0m     Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mforward_by_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# scores\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(Y_pred, torch\u001b[38;5;241m.\u001b[39mfrom_numpy(Y)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     40\u001b[0m     Y_pred \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(Y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# convert to probabilities\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36mforward_by_batches\u001b[0;34m(cnn, X)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m create_dataloader(X, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):  \u001b[38;5;66;03m# do not shuffle here!\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 30\u001b[0m         Y\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m cnn\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     32\u001b[0m Y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(Y)\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mConvBNReLU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kappa_history_test = []\n",
    "loss_history_test = []\n",
    "loss_history_train = []\n",
    "losses = []\n",
    "for i in range(num_epoch):\n",
    "    dataloader = create_dataloader(X_train, Y_train, batch_size, shuffle=True)\n",
    "    for x, target in tqdm(dataloader):\n",
    "        x, target = x.to(device), target.type(torch.int64).to(device)\n",
    "        cnn.zero_grad()\n",
    "        output = cnn(x)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging -- track train loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #       Evaluate performance at the end of each epoch\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    # Logging -- average train loss in this epoch\n",
    "    loss_history_train.append(utils.ewm(losses))\n",
    "\n",
    "    # Logging -- evalutate performance on test set\n",
    "    results = evaluate_model(cnn, X_test, Y_test)\n",
    "    loss_history_test.append(results['loss'])\n",
    "    kappa_history_test.append(results['kappa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss history\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss_history_train, color='C0', label='train loss')\n",
    "ax.plot(loss_history_test, color='C1', label='test loss')\n",
    "ax.set_ylabel('loss (CE)')\n",
    "ax.set_xlabel('epoch')\n",
    "ax = ax.twinx()\n",
    "ax.plot(kappa_history_test, color='C2', label='kappa')\n",
    "ax.set_ylabel('kappa')\n",
    "ax.grid(True)\n",
    "fig.legend()\n",
    "\n",
    "# Report\n",
    "Y_test_pred_lab = le.inverse_transform(results['Y_pred'])  # back to text labels\n",
    "Y_test_lab = le.inverse_transform(Y_test)  # back to text labels\n",
    "print('\\nClassifier performance')\n",
    "print('Out of sample:\\n', metrics.classification_report(Y_test_lab, Y_test_pred_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise**: Try improving the performance of the model. Here are some things to try:\n",
    "- Class balancing\n",
    "- Mode smoothing, HMM (Q: How to estimate the emission matrix?)\n",
    "- Architecture design, optimizer, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to own data\n",
    "We are now going to see how well the model performs on the data your collected yesterday. In order to do this, you need the `.CWA` file from your accelerometer (which you can obtain by plugging your accelerometer into the computer, and finding it in Finder) and the `.csv` file that you obtained using the camera browser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder called `my_data` in this repository for the `.csv` and `.CWA` files, and copy the files into this folder. By calling `ls my_data`, you should see these two files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mCWA-DATA.CWA\u001b[m\u001b[m      ex_annotation.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls my_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next install [actipy](https://github.com/OxWearables/actipy). Actipy requires Java to be installed. If you are working on the virtual machines, this should already be set up. If you are working locally on your computer, you can do this using homebrew:\n",
    "```shell\n",
    "brew install java\n",
    "```\n",
    "Importantly, follow the suggestion from homebrew and and create a symbolic link to the homebrew installation. This command will look like:\n",
    "```shell\n",
    "sudo ln -sfn /opt/homebrew/opt/openjdk/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk.jdk\n",
    "``` \n",
    "Once java has been installed, you can import actipy using `pip`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import actipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can process your accelerometer data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file... Done! (0.39s)\n",
      "Converting to dataframe... Done! (0.03s)\n",
      "Lowpass filter... Done! (0.42s)\n",
      "Getting stationary points... Done! (0.13s)\n",
      "Gravity calibration... Done! (0.03s)\n",
      "Nonwear detection... Done! (0.08s)\n",
      "Skipping resample: Sampling rate is already 100\n",
      "Resampling... Done! (0.00s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-11-28 12:39:55.500</th>\n",
       "      <td>-0.078080</td>\n",
       "      <td>-0.421882</td>\n",
       "      <td>0.750069</td>\n",
       "      <td>21.200001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28 12:39:55.510</th>\n",
       "      <td>-0.128589</td>\n",
       "      <td>-0.490073</td>\n",
       "      <td>0.565105</td>\n",
       "      <td>21.200001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28 12:39:55.520</th>\n",
       "      <td>-0.152424</td>\n",
       "      <td>-0.548723</td>\n",
       "      <td>0.474253</td>\n",
       "      <td>21.200001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28 12:39:55.530</th>\n",
       "      <td>-0.146496</td>\n",
       "      <td>-0.597410</td>\n",
       "      <td>0.486646</td>\n",
       "      <td>21.200001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-28 12:39:55.540</th>\n",
       "      <td>-0.131410</td>\n",
       "      <td>-0.642530</td>\n",
       "      <td>0.533560</td>\n",
       "      <td>21.200001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                x         y         z  temperature\n",
       "time                                                              \n",
       "2022-11-28 12:39:55.500 -0.078080 -0.421882  0.750069    21.200001\n",
       "2022-11-28 12:39:55.510 -0.128589 -0.490073  0.565105    21.200001\n",
       "2022-11-28 12:39:55.520 -0.152424 -0.548723  0.474253    21.200001\n",
       "2022-11-28 12:39:55.530 -0.146496 -0.597410  0.486646    21.200001\n",
       "2022-11-28 12:39:55.540 -0.131410 -0.642530  0.533560    21.200001"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_my_data = \"my_data/CWA-DATA.CWA\" # Make sure you specify the right file name here\n",
    "\n",
    "my_data, info = actipy.read_device(path_to_my_data,\n",
    "                                   lowpass_hz=20,\n",
    "                                   calibrate_gravity=True,\n",
    "                                   detect_nonwear=True,\n",
    "                                   resample_hz=100)\n",
    "my_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_time_col(col):\n",
    "    col = col.str.slice(0,19) # 2022-11-28T12:00:00.000Z remove the three digits after the decimal relating to the images, \n",
    "    col = pd.to_datetime(col, format='%Y-%m-%dT%H:%M:%S') # parse to time\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "      <th>duration</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-28 12:00:00</td>\n",
       "      <td>2022-11-28 12:41:00</td>\n",
       "      <td>3600.009</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-28 12:42:00</td>\n",
       "      <td>2022-11-28 13:00:00</td>\n",
       "      <td>3600.019</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            startTime             endTime  duration annotation\n",
       "0 2022-11-28 12:00:00 2022-11-28 12:41:00  3600.009    running\n",
       "1 2022-11-28 12:42:00 2022-11-28 13:00:00  3600.019    walking"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_my_annots = \"my_data/annotation.csv\" # Make sure you specify the right file name here\n",
    "\n",
    "annots = pd.read_csv(path_to_my_annots)\n",
    "annots[\"startTime\"] = process_time_col(annots[\"startTime\"])\n",
    "annots[\"endTime\"] = process_time_col(annots[\"endTime\"])\n",
    "\n",
    "annots.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data[\"annotation\"] = \"un_annotated\" # initially set data to un_annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write some code to match up the annotations from the `.csv` file to the `.cwa` file. Below is some unoptimised code that just loops through the two files copy annotations if the timestamps match up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/585394 readings processed\n",
      "18000/585394 readings processed\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# check whether the current accel timestamp falls within the time frame of the current annotation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accel_time \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m annot_start_time \u001b[38;5;129;01mand\u001b[39;00m accel_time \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m annot_end_time:\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mmy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmy_i\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mannotation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m annot_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m     my_i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# go to next accel\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m accel_time \u001b[38;5;241m<\u001b[39m annot_start_time:\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/pandas/core/indexing.py:2442\u001b[0m, in \u001b[0;36m_AtIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mloc[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   2440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 2442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/pandas/core/indexing.py:2397\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough indexers for scalar access (setting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2397\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4210\u001b[0m, in \u001b[0;36mDataFrame._set_value\u001b[0;34m(self, index, col, value, takeable)\u001b[0m\n\u001b[1;32m   4208\u001b[0m         icol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(col)\n\u001b[1;32m   4209\u001b[0m         iindex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(index)\n\u001b[0;32m-> 4210\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_setitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43micol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[1;32m   4213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   4214\u001b[0m     \u001b[38;5;66;03m# get_loc might raise a KeyError for missing labels (falling back\u001b[39;00m\n\u001b[1;32m   4215\u001b[0m     \u001b[38;5;66;03m#  to (i)loc will do expansion of the index)\u001b[39;00m\n\u001b[1;32m   4216\u001b[0m     \u001b[38;5;66;03m# column_setitem will do validation that may raise TypeError or ValueError\u001b[39;00m\n\u001b[1;32m   4217\u001b[0m     \u001b[38;5;66;03m# set using a non-recursive method & reset the cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:1389\u001b[0m, in \u001b[0;36mBlockManager.column_setitem\u001b[0;34m(self, loc, idx, value)\u001b[0m\n\u001b[1;32m   1387\u001b[0m col_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miget(loc, track_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1388\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m col_mgr\u001b[38;5;241m.\u001b[39msetitem((idx,), value)\n\u001b[0;32m-> 1389\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:1221\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[0;34m(self, loc, value, inplace)\u001b[0m\n\u001b[1;32m   1219\u001b[0m blk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[blkno]\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blk\u001b[38;5;241m.\u001b[39m_mgr_locs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# TODO: fastest way to check this?\u001b[39;00m\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iset_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblkno\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblkno\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;66;03m# \"List[Union[int, slice, ndarray]]\", variable has type \"Union[int,\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;66;03m# slice, ndarray]\")\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m loc \u001b[38;5;241m=\u001b[39m [loc]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Oxford_Wearables_Activity_Recognition/.venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:1360\u001b[0m, in \u001b[0;36mBlockManager._iset_single\u001b[0;34m(self, loc, value, inplace, blkno, blk)\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_reference_block(blkno)\n\u001b[1;32m   1359\u001b[0m     iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblklocs[loc]\n\u001b[0;32m-> 1360\u001b[0m     \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miloc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m nb \u001b[38;5;241m=\u001b[39m new_block_2d(value, placement\u001b[38;5;241m=\u001b[39mblk\u001b[38;5;241m.\u001b[39m_mgr_locs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Add annotations to data\n",
    "# first sort the two data-frames\n",
    "my_data = my_data.sort_values(\"time\")\n",
    "annots = annots.sort_values(\"startTime\")\n",
    "\n",
    "my_i = 0\n",
    "an_i = 0\n",
    "while(my_i<len(my_data) and an_i<len(annots)):\n",
    "    accel_time = my_data.index[my_i]\n",
    "    annot_start_time = annots[\"startTime\"][an_i]\n",
    "    annot_end_time = annots[\"endTime\"][an_i]\n",
    "    \n",
    "    if not (my_i % 1000):\n",
    "        print(f\"{my_i}/{len(my_data)} readings processed\", end=\"\\r\")\n",
    "\n",
    "    # check whether the current accel timestamp falls within the time frame of the current annotation\n",
    "    if accel_time >= annot_start_time and accel_time <= annot_end_time:\n",
    "        my_data.at[ my_data.index[my_i], \"annotation\" ] = annot_row[\"annotation\"]\n",
    "        my_i += 1 # go to next accel\n",
    "    \n",
    "    elif accel_time < annot_start_time:\n",
    "        my_i += 1 # go to next accel \n",
    "        \n",
    "    \n",
    "    elif accel_time > annot_end_time:\n",
    "        an_i += 1 # go to next label\n",
    "        \n",
    "    else: # shouldn't get here\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to process the annotated accelerometer data so that it is in the same shape as the data fed into the CNN. To do this, we need to break it up into windows first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_X, my_Y, my_T = utils.make_windows(my_data, winsec=30, sample_rate=100, dropna=False, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can put our data into the model, transform the outputs to probabilities, and look at which class has the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mixed', 'mixed', 'sit-stand', 'sleep', 'sleep', 'sleep', 'sleep',\n",
       "       'sleep', 'sleep', 'sleep', 'sleep', 'sleep', 'sleep', 'sleep',\n",
       "       'sleep', 'sleep', 'sleep', 'sleep', 'sleep', 'sleep', 'sleep',\n",
       "       'sleep', 'sleep', 'sleep', 'sleep', 'sleep', 'sleep', 'sleep',\n",
       "       'sleep', 'sleep', 'sleep', 'sleep', 'sleep', 'sleep', 'sleep',\n",
       "       'sleep', 'sleep', 'sleep', 'sleep', 'sleep', 'sleep', 'sleep',\n",
       "       'sleep', 'sleep', 'sleep', 'sleep', 'sleep', 'sleep', 'sleep',\n",
       "       'sleep', 'sleep', 'sleep', 'sit-stand'], dtype=object)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = forward_by_batches(cnn, my_X)\n",
    "Y_pred = F.softmax(Y_pred, dim=1)  # convert to probabilities\n",
    "Y_pred = torch.argmax(Y_pred, dim=1)  # convert to classes\n",
    "Y_pred = Y_pred.cpu().numpy()  # cast to numpy array\n",
    "le.inverse_transform(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these match up with your annotations? Note: you may have to simplify your annoations using `anno_label_dict.loc[my_Y, 'label:Willetts2018'].to_numpy()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['running', 'running', 'un_annotated', 'un_annotated', 'running',\n",
       "       'running', 'un_annotated', 'un_annotated', 'un_annotated',\n",
       "       'un_annotated', 'un_annotated', 'un_annotated', 'un_annotated',\n",
       "       'un_annotated', 'un_annotated', 'un_annotated', 'un_annotated',\n",
       "       'un_annotated', 'un_annotated', 'un_annotated', 'un_annotated',\n",
       "       'un_annotated', 'un_annotated', 'un_annotated', 'un_annotated',\n",
       "       'un_annotated', 'un_annotated', 'un_annotated', 'un_annotated',\n",
       "       'un_annotated', 'un_annotated', 'un_annotated', 'un_annotated',\n",
       "       'un_annotated', 'un_annotated', 'un_annotated', 'un_annotated',\n",
       "       'un_annotated', 'un_annotated', 'un_annotated', 'un_annotated',\n",
       "       'un_annotated', 'un_annotated', 'un_annotated', 'un_annotated',\n",
       "       'un_annotated', 'un_annotated', 'un_annotated', 'un_annotated',\n",
       "       'un_annotated', 'un_annotated', 'un_annotated', 'un_annotated'],\n",
       "      dtype='<U12')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba62a1b4a2d59cbe868c83bdd535aa95e1f6d51e6a8dbfe9705911f17ded0148"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
