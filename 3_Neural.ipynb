{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Neural networks in activity recognition\n","\n","Engineering effective features is one of the most time-consuming parts of\n","machine learning.\n","The appeal of neural networks is that feature enginnering is integrated into the\n","training process &mdash; they automatically engineer features that are relevant\n","for the learning task directly from the raw representation of the data\n","\n","## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","from sklearn import metrics\n","from sklearn.preprocessing import LabelEncoder\n","from tqdm.auto import tqdm\n","\n","import utils\n","\n","# For reproducibility\n","np.random.seed(42)\n","torch.manual_seed(42)\n","cudnn.benchmark = True\n","\n","# Grab a GPU if there is one\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"Using {} device: {}\".format(device, torch.cuda.current_device()))\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Using {}\".format(device))"]},{"cell_type":"markdown","metadata":{},"source":["## Load dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Path to your extracted windows\n","DATASET_PATH = 'processed_data/'\n","print(f'Content of {DATASET_PATH}')\n","print(os.listdir(DATASET_PATH))\n","\n","X = np.load(DATASET_PATH+'X.npy', mmap_mode='r')\n","Y = np.load(DATASET_PATH+'Y.npy')\n","T = np.load(DATASET_PATH+'T.npy')\n","pid = np.load(DATASET_PATH+'pid.npy')\n","\n","# As before, let's map the text annotations to simplified labels\n","ANNO_LABEL_DICT_PATH = 'capture24/annotation-label-dictionary.csv'\n","anno_label_dict = pd.read_csv(ANNO_LABEL_DICT_PATH, index_col='annotation', dtype='string')\n","Y = anno_label_dict.loc[Y, 'label:Willetts2018'].to_numpy()\n","\n","# Transform to numeric\n","le = LabelEncoder().fit(Y)\n","Y = le.transform(Y)"]},{"cell_type":"markdown","metadata":{},"source":["## Train/test split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Hold out participants P101-P151 for testing (51 participants)\n","test_ids = [f'P{i}' for i in range(101,152)]\n","mask_test = np.isin(pid, test_ids)\n","mask_train = ~mask_test\n","X_train, Y_train, T_train, pid_train = \\\n","    X[mask_train], Y[mask_train], T[mask_train], pid[mask_train]\n","X_test, Y_test, T_test, pid_test = \\\n","    X[mask_test], Y[mask_test], T[mask_test], pid[mask_test]\n","print(\"Shape of X_train:\", X_train.shape)\n","print(\"Shape of X_test:\", X_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Architecture design\n","\n","As a baseline, let's use a convolutional neural network (CNN) with a\n","typical pyramid-like structure. The input to the network is a `(N,3,3000)`\n","array, corresponding to `N` windows of raw tri-axial accelerometer measures.\n","Note the transposed format `(3,3000)` instead of `(3000,3)`; this *channels\n","first* format is the default in PyTorch.\n","\n","The output of the CNN is a `(N,num_labels)` array where each row contains\n","predicted unnormalized class scores or *logits*; pass each row to a softmax\n","if you want to convert it to probabilities."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ConvBNReLU(nn.Module):\n","    ''' Convolution + batch normalization + ReLU is a common trio '''\n","    def __init__(\n","        self, in_channels, out_channels,\n","        kernel_size=3, stride=1, padding=1, bias=True\n","    ):\n","        super(ConvBNReLU, self).__init__()\n","\n","        self.main = nn.Sequential(\n","            nn.Conv1d(in_channels, out_channels,\n","                kernel_size, stride, padding, bias=bias),\n","            nn.BatchNorm1d(out_channels),\n","            nn.ReLU(True)\n","        )\n","\n","    def forward(self, x):\n","        return self.main(x)\n","\n","\n","class CNN(nn.Module):\n","    ''' Typical CNN design with pyramid-like structure '''\n","    def __init__(self, output_size=5, in_channels=3, num_filters_init=8):\n","        super(CNN, self).__init__()\n","\n","        self.cnn = nn.Sequential(\n","            ConvBNReLU(in_channels, num_filters_init,\n","            8, 4, 2, bias=False),  # 1500 -> 750\n","            ConvBNReLU(num_filters_init, num_filters_init*2,\n","            6, 4, 2, bias=False),  # 750 -> 188\n","            ConvBNReLU(num_filters_init*2, num_filters_init*4,\n","            8, 4, 2, bias=False),  # 188 -> 47\n","            ConvBNReLU(num_filters_init*4, num_filters_init*8,\n","            3, 2, 1, bias=False),  # 47 -> 24\n","            ConvBNReLU(num_filters_init*8, num_filters_init*16,\n","            4, 2, 1, bias=False),  # 24 -> 12\n","            ConvBNReLU(num_filters_init*16, num_filters_init*32,\n","            4, 2, 1, bias=False),  # 12 -> 6\n","            ConvBNReLU(num_filters_init*32, num_filters_init*64,\n","            6, 1, 0, bias=False),  # 6 -> 1\n","            nn.Conv1d(num_filters_init*64, output_size,\n","            1, 1, 0, bias=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.cnn(x).view(x.shape[0],-1)"]},{"cell_type":"markdown","metadata":{},"source":["## Helper functions\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_dataloader(X, y=None, batch_size=1, shuffle=False):\n","    ''' Create a (batch) iterator over the dataset. Alternatively, you can use\n","    PyTorch's Dataset and DataLoader classes -- See\n","    https://pytorch.org/tutorials/beginner/data_loading_tutorial.html '''\n","    if shuffle:\n","        idxs = np.random.permutation(np.arange(len(X)))\n","    else:\n","        idxs = np.arange(len(X))\n","    for i in range(0, len(idxs), batch_size):\n","        idxs_batch = idxs[i:i+batch_size]\n","        X_batch = X[idxs_batch].astype('f4')  # PyTorch defaults to float32\n","        X_batch = np.transpose(X_batch, (0,2,1))  # channels first: (N,M,3) -> (N,3,M). PyTorch uses channel first format\n","        X_batch = torch.from_numpy(X_batch)\n","        if y is None:\n","            yield X_batch\n","        else:\n","            y_batch = y[idxs_batch]\n","            y_batch = torch.from_numpy(y_batch)\n","            yield X_batch, y_batch\n","\n","\n","def forward_by_batches(cnn, X):\n","    ''' Forward pass model on a dataset.\n","    Do this by batches so that we don't blow up the memory. '''\n","    Y = []\n","    cnn.eval()\n","    with torch.no_grad():\n","        for x in create_dataloader(X, batch_size=1024, shuffle=False):  # do not shuffle here!\n","            x = x.to(device)\n","            Y.append(cnn(x))\n","    cnn.train()\n","    Y = torch.cat(Y)\n","    return Y\n","\n","\n","def evaluate_model(cnn, X, Y):\n","    Y_pred = forward_by_batches(cnn, X)  # scores\n","    loss = F.cross_entropy(Y_pred, torch.from_numpy(Y).type(torch.int64).to(device)).item()\n","\n","    Y_pred = F.softmax(Y_pred, dim=1)  # convert to probabilities\n","    Y_pred = torch.argmax(Y_pred, dim=1)  # convert to classes\n","    Y_pred = Y_pred.cpu().numpy()  # cast to numpy array\n","    kappa = metrics.cohen_kappa_score(Y, Y_pred)\n","\n","    return {'loss':loss, 'kappa':kappa, 'Y_pred':Y_pred}"]},{"cell_type":"markdown","metadata":{},"source":[" ## Hyperparameters, model instantiation, loss function and optimizer "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_filters_init = 32  # initial num of filters -- see class definition\n","in_channels = 3  # num of channels of the signal -- equal to 3 for our raw triaxial timeseries\n","output_size = len(np.unique(Y))  # num of classes (sleep, sedentary, etc...)\n","num_epoch = 5  # num of epochs (full loops though the training set)\n","lr = 3e-4  # learning rate\n","batch_size = 32  # size of the mini-batch\n","\n","cnn = CNN(\n","    output_size=output_size,\n","    in_channels=in_channels,\n","    num_filters_init=num_filters_init\n",").to(device)\n","print(cnn)\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(cnn.parameters(), lr=lr)"]},{"cell_type":"markdown","metadata":{},"source":["## Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kappa_history_test = []\n","loss_history_test = []\n","loss_history_train = []\n","losses = []\n","for i in tqdm(range(num_epoch)):\n","    dataloader = create_dataloader(X_train, Y_train, batch_size, shuffle=True)\n","    for x, target in dataloader:\n","        x, target = x.to(device), target.type(torch.int64).to(device)\n","        cnn.zero_grad()\n","        output = cnn(x)\n","        loss = loss_fn(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Logging -- track train loss\n","        losses.append(loss.item())\n","\n","    # --------------------------------------------------------\n","    #       Evaluate performance at the end of each epoch\n","    # --------------------------------------------------------\n","\n","    # Logging -- average train loss in this epoch\n","    loss_history_train.append(utils.ewm(losses))\n","\n","    # Logging -- evalutate performance on test set\n","    results = evaluate_model(cnn, X_test, Y_test)\n","    loss_history_test.append(results['loss'])\n","    kappa_history_test.append(results['kappa'])"]},{"cell_type":"markdown","metadata":{},"source":[" ## Model performane "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loss history\n","plt.close('all')\n","fig, ax = plt.subplots()\n","ax.plot(loss_history_train, color='C0', label='train loss')\n","ax.plot(loss_history_test, color='C1', label='test loss')\n","ax.set_ylabel('loss (CE)')\n","ax.set_xlabel('epoch')\n","ax = ax.twinx()\n","ax.plot(kappa_history_test, color='C2', label='kappa')\n","ax.set_ylabel('kappa')\n","ax.grid(True)\n","fig.legend()\n","\n","# Report\n","Y_test_pred_lab = le.inverse_transform(results['Y_pred'])  # back to text labels\n","Y_test_lab = le.inverse_transform(Y_test)  # back to text labels\n","print('\\nClassifier performance')\n","print('Out of sample:\\n', metrics.classification_report(Y_test_lab, Y_test_pred_lab))"]},{"cell_type":"markdown","metadata":{},"source":["**Excercise**: Try improving the performance of the model. Here are some things to try:\n","- Class balancing\n","- Mode smoothing, HMM (Q: How to estimate the emission matrix?)\n","- Architecture design, optimizer, etc."]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('wearables_workshop')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"ba62a1b4a2d59cbe868c83bdd535aa95e1f6d51e6a8dbfe9705911f17ded0148"}}},"nbformat":4,"nbformat_minor":2}
