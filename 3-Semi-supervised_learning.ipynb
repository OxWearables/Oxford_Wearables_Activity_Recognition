{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity recognition on the Capture24 dataset\n",
    "\n",
    "## Semi-supervised learning\n",
    "\n",
    "While digital data collection is becoming easier and cheaper, labeling such data\n",
    "still requires expensive and time-consuming human labor.\n",
    "For example, while it is possible to label accelerometer measurements for ~150\n",
    "participants as in our Capture24 dataset, it is unfeasible to do so for the\n",
    "tens of thousands of *unlabeled* accelerometer measurements that are\n",
    "currently available in the [UK\n",
    "Biobank](https://www.ukbiobank.ac.uk/activity-monitor-3/) because *a)*\n",
    "compliance to wear body cameras is much lower than wrist-worn accelerometers\n",
    "and *b)* the human labor to go through all the camera recordings would be\n",
    "very expensive. Semi-supervised learning is therefore of great interest,\n",
    "where the aim is to somehow use the unlabeled data to improve the model\n",
    "performance.\n",
    "\n",
    "###### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "import utils\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###### Load dataset and hold out some instances for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of capture24.npz: ['X_feats', 'y', 'pid', 'time', 'annotation']\n",
      "Shape of X_train: (325619, 125)\n",
      "Shape of X_test: (4991, 125)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('capture24.npz', allow_pickle=True)\n",
    "# data = np.load('capture24_small.npz', allow_pickle=True)\n",
    "print(\"Contents of capture24.npz:\", data.files)\n",
    "X, y, pid, time = data['X_feats'], data['y'], data['pid'], data['time']\n",
    "\n",
    "# Hold out some participants for testing the model\n",
    "test_pids = [2, 3]\n",
    "test_mask = np.isin(pid, test_pids)\n",
    "train_mask = ~np.isin(pid, test_pids)\n",
    "X_train, y_train, pid_train = X[train_mask], y[train_mask], pid[train_mask]\n",
    "X_test, y_test, pid_test = X[test_mask], y[test_mask], pid[test_mask]\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-training\n",
    "\n",
    "One of the simplest semi-supervised methods is based on proxy-labels via self-training. The idea is to simply evaluate a trained model on the unlabeled instances and incorporate those with high confidence predictions into the training set, then re-train the model on the augmented set. This process is repeated several times until some criteria is met, e.g. when no more instances are being included in the training set.\n",
    "This simple technique works well when the initial model is already very\n",
    "strong. If the initial model is weak, however, it may reinforce the mistakes\n",
    "in its predictions.\n",
    "In the following, we first train a random forest on the labelled training\n",
    "set, then evaluate the model on the provided unlabelled dataset\n",
    "`capture24_test.npz` for self-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of capture24_test.npz: ['X_feats', 'pid', 'time']\n",
      "Shape of X_unl: (38750, 125)\n"
     ]
    }
   ],
   "source": [
    "# initial model\n",
    "classifier = RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=4)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Load unlabelled dataset for self-training\n",
    "data_unl = np.load('capture24_test.npz')\n",
    "print(\"\\nContents of capture24_test.npz:\", data_unl.files)\n",
    "X_unl = data_unl['X_feats']\n",
    "print(\"Shape of X_unl:\", X_unl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Self-training\n",
    "\n",
    "*Note: this takes several minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b88fda23ab4d34ae39bdea25345b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 18792 instances from the unlabeled set\n",
      "Using 20171 instances from the unlabeled set\n",
      "Using 20893 instances from the unlabeled set\n",
      "Using 21378 instances from the unlabeled set\n",
      "Using 21725 instances from the unlabeled set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial predictions and self-training parameters\n",
    "y_unl_pred = classifier.predict(X_unl)\n",
    "y_unl_prob = classifier.predict_proba(X_unl)\n",
    "y_unl_pred_old = None\n",
    "max_iter = 5\n",
    "prob_threshold = 0.8\n",
    "\n",
    "for i in tqdm(range(max_iter)):\n",
    "\n",
    "    if np.array_equal(y_unl_pred, y_unl_pred_old):\n",
    "        tqdm.write(\"Iteration stopped: no more change found in self-training\")\n",
    "        break\n",
    "\n",
    "    y_unl_pred_old = np.copy(y_unl_pred)\n",
    "    confident_mask = np.any(y_unl_prob > prob_threshold, axis=1)\n",
    "    tqdm.write(f\"Using {np.sum(confident_mask)} instances from the unlabeled set\")\n",
    "\n",
    "    # re-train on augmented set\n",
    "    classifier.fit(\n",
    "        np.vstack((X_train, X_unl[confident_mask])),\n",
    "        np.hstack((y_train, y_unl_pred_old[confident_mask]))\n",
    "    )\n",
    "\n",
    "    # updated predictions\n",
    "    y_unl_pred = classifier.predict(X_unl)\n",
    "    y_unl_prob = classifier.predict_proba(X_unl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###### Smooth the predictions via HMM and evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random forest performance with self-training and HMM smoothing ---\n",
      "Accuracy score: 0.9004207573632539\n",
      "Balanced accuracy score: 0.6391289988205523\n",
      "Cohen kappa score: 0.8404686883093087\n",
      "\n",
      "Per-class recall scores:\n",
      "sleep      : 0.9766597510373444\n",
      "sedentary  : 0.9626294461954075\n",
      "tasks-light: 0.0\n",
      "walking    : 0.36774193548387096\n",
      "moderate   : 0.8886138613861386\n",
      "\n",
      "Confusion matrix:\n",
      " [[1883   45    0    0    0]\n",
      " [  53 2138    0   18   12]\n",
      " [   0  111    0    9    8]\n",
      " [   0   91    0  114  105]\n",
      " [   0   41    0    4  359]]\n"
     ]
    }
   ],
   "source": [
    "Y_oob = classifier.oob_decision_function_[:y_train.shape[0]]\n",
    "prior, emission, transition = utils.train_hmm(Y_oob, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "y_test_hmm = utils.viterbi(y_test_pred, prior, emission, transition)\n",
    "print(\"\\n--- Random forest performance with self-training and HMM smoothing ---\")\n",
    "utils.print_scores(utils.compute_scores(y_test, y_test_hmm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ideas\n",
    "\n",
    "- Tune acceptance threshold of high confidence predictions.\n",
    "- Incorporate the HMM smoothing into the self-training loop.\n",
    "\n",
    "###### References\n",
    "\n",
    "- [A nice summary of proxy-labels methods](https://ruder.io/semi-supervised/)\n",
    "- [Semi-supervised methods in sklearn](https://scikit-learn.org/stable/modules/label_propagation.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
